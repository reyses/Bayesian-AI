diff --git a/core/data_aggregator.py b/core/data_aggregator.py
index 298d42e..cc1be01 100644
--- a/core/data_aggregator.py
+++ b/core/data_aggregator.py
@@ -11,7 +11,10 @@ class DataAggregator:
         self.max_ticks = max_ticks
         self.ticks: List[Dict] = []
         self._df_cache: Optional[pd.DataFrame] = None
-        self._last_tick_count = 0
+        self._bars_cache: Dict[str, pd.DataFrame] = {}
+        # Track total ticks added to manage incremental updates
+        self._total_ticks_added = 0
+        self._last_processed_ticks = 0

     def add_tick(self, tick: Dict):
         """
@@ -23,12 +26,11 @@ class DataAggregator:
             tick['timestamp'] = pd.Timestamp.now().timestamp()

         self.ticks.append(tick)
+        self._total_ticks_added += 1
+
         if len(self.ticks) > self.max_ticks:
             self.ticks.pop(0)

-        # Invalidate cache
-        self._df_cache = None
-
     def get_current_data(self) -> Dict:
         """
         Get snapshot of data for LayerEngine
@@ -45,39 +47,101 @@ class DataAggregator:
                 'bars_4hr': None
             }

-        # Convert to DataFrame
+        # Initialize or Update _df_cache
         if self._df_cache is None:
+            # Initial build
             self._df_cache = pd.DataFrame(self.ticks)
-            # Ensure timestamp column is datetime for resampling
-            if not pd.api.types.is_datetime64_any_dtype(self._df_cache['timestamp']):
-                # Assuming timestamp is float (epoch)
-                self._df_cache['datetime'] = pd.to_datetime(self._df_cache['timestamp'], unit='s')
-            else:
-                self._df_cache['datetime'] = self._df_cache['timestamp']
-
-            self._df_cache.set_index('datetime', inplace=True)
+            self._prepare_df_index(self._df_cache)
+            self._last_processed_ticks = self._total_ticks_added
+        else:
+            # Incremental update
+            new_count = self._total_ticks_added - self._last_processed_ticks
+
+            if new_count > 0:
+                if new_count >= len(self.ticks):
+                    # Should be rare/impossible unless max_ticks is huge or logic error,
+                    # but if we need to add more than we have, just rebuild.
+                    self._df_cache = pd.DataFrame(self.ticks)
+                    self._prepare_df_index(self._df_cache)
+                else:
+                    # Append new ticks
+                    new_ticks_data = self.ticks[-new_count:]
+                    new_df = pd.DataFrame(new_ticks_data)
+                    self._prepare_df_index(new_df)
+                    self._df_cache = pd.concat([self._df_cache, new_df])
+
+                # Maintain cache size
+                if len(self._df_cache) > self.max_ticks:
+                    self._df_cache = self._df_cache.iloc[-self.max_ticks:]
+
+                self._last_processed_ticks = self._total_ticks_added

         df = self._df_cache
         current_price = self.ticks[-1]['price']
         current_ts = self.ticks[-1]['timestamp']

-        # Generate bars
-        # Note: In a real high-freq system, we wouldn't resample full history every tick.
-        # We would update the last bar. This is a simplified implementation.
-
-        # Helper to safely resample
+        # Helper to safely resample with caching
         def get_bars(rule):
             try:
                 if df.empty: return None
-                resampled = df.resample(rule).agg({
-                    'open': 'first',
-                    'high': 'max',
-                    'low': 'min',
-                    'close': 'last',
-                    'volume': 'sum'
-                }).dropna()
-                return resampled
+
+                # Use cache if available
+                cached = self._bars_cache.get(rule)
+
+                start_subset_idx = None
+
+                if cached is not None and not cached.empty:
+                    # Identify the last bar in the cache
+                    last_bar_idx = cached.index[-1]
+
+                    # Check if last_bar_idx is still within df range
+                    if last_bar_idx <= df.index[-1]:
+                         start_subset_idx = last_bar_idx
+
+                if start_subset_idx is not None:
+                    # Incremental update
+                    # Get relevant data slice
+                    subset = df[df.index >= start_subset_idx]
+
+                    if subset.empty:
+                        return cached
+
+                    resampled_subset = subset.resample(rule).agg({
+                        'open': 'first',
+                        'high': 'max',
+                        'low': 'min',
+                        'close': 'last',
+                        'volume': 'sum'
+                    }).dropna()
+
+                    if resampled_subset.empty:
+                        return cached
+
+                    # Merge: Cache (minus last bar) + Resampled Subset
+                    base_cache = cached.loc[:start_subset_idx]
+                    if not base_cache.empty and base_cache.index[-1] == start_subset_idx:
+                         base_cache = base_cache.iloc[:-1]
+
+                    updated_cache = pd.concat([base_cache, resampled_subset])
+                else:
+                    # Full resample (initial or cache invalid)
+                    updated_cache = df.resample(rule).agg({
+                        'open': 'first',
+                        'high': 'max',
+                        'low': 'min',
+                        'close': 'last',
+                        'volume': 'sum'
+                    }).dropna()
+
+                # Trim cache to match the window of ticks we hold
+                if len(updated_cache) > self.max_ticks:
+                    updated_cache = updated_cache.iloc[-self.max_ticks:]
+
+                self._bars_cache[rule] = updated_cache
+                return updated_cache
+
             except Exception:
+                # In case of missing columns or other errors
                 return None

         return {
@@ -89,3 +153,18 @@ class DataAggregator:
             'bars_1h': get_bars('1h'),
             'bars_4hr': get_bars('4h')
         }
+
+    def _prepare_df_index(self, df: pd.DataFrame):
+        """Helper to ensure datetime index exists"""
+        if 'datetime' not in df.columns:
+            if 'timestamp' in df.columns:
+                if not pd.api.types.is_datetime64_any_dtype(df['timestamp']):
+                     df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')
+                else:
+                     df['datetime'] = df['timestamp']
+            else:
+                # Should not happen given add_tick logic
+                pass
+
+        if 'datetime' in df.columns:
+            df.set_index('datetime', inplace=True)
