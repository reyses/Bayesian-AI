name: Data Preprocessing Pipeline

on:
  # Manual trigger
  workflow_dispatch:
    inputs:
      data_source:
        description: 'Data source directory'
        required: true
        default: 'DATA/RAW'
      output_dir:
        description: 'Output directory for parquet'
        required: true
        default: 'DATA/PARQUET'
  
  # Auto-run when new data is pushed
  push:
    paths:
      - 'DATA/RAW/**/*.dbn.zst'
      - 'DATA/RAW/**/*.dbn'

jobs:
  preprocess-data:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        lfs: true  # If using Git LFS for large files
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        pip install --upgrade pip
        pip install databento zstandard pandas pyarrow
    
    - name: Check data files
      run: |
        echo "=== Checking for DBN files ==="
        find DATA/RAW -name "*.dbn*" -type f | head -20
        echo "Total DBN files: $(find DATA/RAW -name "*.dbn*" -type f | wc -l)"
    
    - name: Convert DBN to Parquet
      run: |
        echo "=== Converting DBN to Parquet ==="
        mkdir -p DATA/PARQUET
        
        python training/data_loading_optimizer.py convert \
          DATA/RAW/ \
          DATA/PARQUET/
    
    - name: Verify conversion
      run: |
        echo "=== Verifying Parquet files ==="
        find DATA/PARQUET -name "*.parquet" -type f
        echo "Total Parquet files: $(find DATA/PARQUET -name "*.parquet" -type f | wc -l)"
        
        # Show file sizes
        du -sh DATA/RAW/ DATA/PARQUET/
    
    - name: Cache Parquet files
      uses: actions/cache@v4
      with:
        path: DATA/PARQUET
        key: parquet-data-${{ hashFiles('DATA/RAW/**/*.dbn*') }}
        restore-keys: |
          parquet-data-
    
    - name: Upload Parquet artifacts
      uses: actions/upload-artifact@v4
      with:
        name: parquet-data
        path: DATA/PARQUET/**/*.parquet
        retention-days: 30
        compression-level: 0  # Already compressed
    
    - name: Performance report
      run: |
        echo "=== Performance Report ===" > preprocessing_report.txt
        echo "DBN files: $(find DATA/RAW -name "*.dbn*" | wc -l)" >> preprocessing_report.txt
        echo "Parquet files: $(find DATA/PARQUET -name "*.parquet" | wc -l)" >> preprocessing_report.txt
        echo "DBN size: $(du -sh DATA/RAW/ | cut -f1)" >> preprocessing_report.txt
        echo "Parquet size: $(du -sh DATA/PARQUET/ | cut -f1)" >> preprocessing_report.txt
        cat preprocessing_report.txt
    
    - name: Upload report
      uses: actions/upload-artifact@v4
      with:
        name: preprocessing-report
        path: preprocessing_report.txt


  # Optional: Run training with preprocessed data
  test-training:
    needs: preprocess-data
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
    
    - name: Download preprocessed data
      uses: actions/download-artifact@v4
      with:
        name: parquet-data
        path: DATA/PARQUET
    
    - name: Restore cache
      uses: actions/cache@v4
      with:
        path: DATA/PARQUET
        key: parquet-data-${{ hashFiles('DATA/RAW/**/*.dbn*') }}
    
    - name: Test data loading speed
      run: |
        python -c "
        import time
        import pandas as pd
        import glob
        
        files = glob.glob('DATA/PARQUET/*.parquet')
        print(f'Loading {len(files)} parquet files...')
        
        start = time.time()
        dfs = [pd.read_parquet(f) for f in files]
        data = pd.concat(dfs, ignore_index=True)
        elapsed = time.time() - start
        
        print(f'Loaded {len(data):,} rows in {elapsed:.2f}s')
        print(f'Speed: {len(data)/elapsed:,.0f} rows/sec')
        "
    
    - name: Run quick training test (optional)
      run: |
        # Run 10 iterations to verify system works
        python training/orchestrator.py \
          --data DATA/PARQUET \
          --iterations 10 \
          --mode QUICK_TEST
